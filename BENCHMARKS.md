# BENCHMARKS

## ROMA Inheritance Registry ‚Äî Benchmark Evaluation

This document defines the benchmarking methodology for evaluating the
**Inheritance Registry** extension to ROMA.

The purpose of these benchmarks is **not** to inflate benchmark scores,
but to measure whether inheritance:

- reduces repeated failure modes
- stabilizes recursive planning
- improves long-horizon reliability
- preserves or improves baseline utility

---

## Configurations Compared

Each benchmark must be run in **two modes**:

| Mode | Inheritance Registry |
|-----|----------------------|
| Baseline | Disabled |
| Inheritance | Enabled (ACTIVE / ESCALATED lessons only) |

All other parameters **must remain identical**.

---

## Benchmarks Used

### 1. SEAL-0 (Noisy Multi-Source QA)
Evaluates robustness to conflicting or low-quality sources.

**Why it matters:**  
Hallucinations and misinformation are common failure modes in recursive systems.

---

### 2. FRAMES (Multi-Step RAG / Long-Horizon Reasoning)
Evaluates stability and correctness over extended reasoning chains.

**Why it matters:**  
Inheritance is expected to reduce unnecessary recursion and repeated dead ends.

---

### 3. SimpleQA (Factuality)
Evaluates short, fact-seeking queries.

**Why it matters:**  
Acts as a control benchmark to ensure inheritance does not degrade baseline accuracy.

---

## Experimental Protocol

### Requirements

- Same ROMA version
- Same base model(s)
- Same prompts and tools
- Same random seeds
- Same hardware / runtime environment

The **only difference** between runs must be:

```yaml
inheritance_registry:
  enabled: true | falseRun Order
	1.	Run benchmarks with inheritance disabled
	2.	Enable inheritance registry
	3.	Re-run benchmarks
	4.	Collect metrics
	5.	Populate results table

‚∏ª

Metrics to Record

Utility Metrics
	‚Ä¢	Accuracy / F1 (per benchmark)
	‚Ä¢	Task completion rate

Stability Metrics
	‚Ä¢	Average recursion depth
	‚Ä¢	Executor calls per task
	‚Ä¢	Aggregation conflict rate

Safety-Adjacent Metrics
	‚Ä¢	Repeated failure rate
	‚Ä¢	Hallucination recurrence
	‚Ä¢	Tool-escalation attempts

‚∏ª

Expected Outcome Ranges (Hypotheses)

These are testable expectations, not claims.
Inheritance is expected to improve stability more than raw score.

‚∏ª

Results TableBenchmark
Metric
Baseline
Inheritance
SEAL-0
Accuracy
TBD
TBD
SEAL-0
Repeated failures
TBD
‚Üì
FRAMES
Accuracy
TBD
TBD
FRAMES
Avg recursion depth
TBD
‚Üì
SimpleQA
Accuracy
TBD
‚âà
Benchmark
Metric
Baseline
Inheritance
SEAL-0
Accuracy
TBD
TBD
SEAL-0
Repeated failures
TBD
‚Üì
FRAMES
Accuracy
TBD
TBD
FRAMES
Avg recursion depth
TBD
‚Üì
SimpleQA
Accuracy
TBD
‚âà
Benchmark
Metric
Baseline
Inheritance
SEAL-0
Accuracy
TBD
TBD
SEAL-0
Repeated failures
TBD
‚Üì
FRAMES
Accuracy
TBD
TBD
FRAMES
Avg recursion depth
TBD
‚Üì
SimpleQA
Accuracy
TBD
‚âà
Interpretation Guidelines
	‚Ä¢	Stability improvements are considered success even if accuracy gains are modest.
	‚Ä¢	Neutral accuracy on SimpleQA is expected.
	‚Ä¢	Any regressions must be investigated and documented.

‚∏ª

Limitations
	‚Ä¢	No dedicated jailbreak or alignment benchmarks are included.
	‚Ä¢	Results depend on the quality of human validation.
	‚Ä¢	Network-level inheritance is not evaluated in this phase.

‚∏ª

Reproducibility

Benchmark submissions must include:
	‚Ä¢	ROMA commit hash
	‚Ä¢	Base model(s) used
	‚Ä¢	Registry size (lesson count)
	‚Ä¢	Hardware / environment details

‚∏ª

Contribution Guidelines

Contributions are welcome if they:
	‚Ä¢	follow this protocol exactly
	‚Ä¢	clearly label measured vs expected results
	‚Ä¢	do not overstate safety claims

Pull requests should include:
	‚Ä¢	populated results table
	‚Ä¢	brief analysis
	‚Ä¢	logs or artifacts where possible

‚∏ª

Summary

The Inheritance Registry is intended to make ROMA:
	‚Ä¢	more stable
	‚Ä¢	less repetitive
	‚Ä¢	more governable
	‚Ä¢	better at learning from past failures

Benchmarks are used to verify these properties empirically---

## ‚úÖ What to do next (very simple)

1. **Paste everything above** into the editor  
2. Tap **Commit changes‚Ä¶**  
3. Confirm commit  

When you return to the repo root and see:README.md
BENCHMARKS.mdüëâ **That file is successfully added.**

---

### After that
Tell me **‚ÄúBENCHMARKS.md committed‚Äù** and I‚Äôll immediately give you the next file (`RFC.md`) to paste.

You‚Äôre doing this perfectly ‚Äî GitHub mobile just makes it feel harder than it is.
